{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e8b743",
   "metadata": {},
   "source": [
    "### [IN PROGRESS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30971007",
   "metadata": {},
   "source": [
    "Sources used in this notebook:\\\n",
    "    1. [This google colab that implements a simpler version](https://colab.research.google.com/github/dzlab/notebooks/blob/master/_notebooks/2022-02-27-Swin_Transfomer.ipynb#scrollTo=-UnAWaONhf9J)\\\n",
    "    2. [The model outlined in the original research paper](https://arxiv.org/pdf/2103.14030.pdf)\\\n",
    "    3. [To load the weights of the pretrained model](https://tfhub.dev/sayakpaul/swin_s3_tiny_224/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbae09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention,\n",
    "    Softmax\n",
    ")\n",
    "from tensorflow.keras.initializers import TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b56833",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.path.dirname(os.path.abspath('brain_MRI_classification.ipynb'))\n",
    "datasets_combined = os.path.join(notebook_path, 'brainMRI_data')\n",
    "\n",
    "train_directory = os.path.join(datasets_combined, 'Training')\n",
    "test_directory = os.path.join(datasets_combined, 'Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52809fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 files belonging to 4 classes.\n",
      "Using 2296 files for training.\n",
      "Found 2870 files belonging to 4 classes.\n",
      "Using 574 files for validation.\n",
      "Found 394 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# using 'int' to use sparse_categorical_crossentropy for loss\n",
    "train_dataset = image_dataset_from_directory(train_directory,\n",
    "                                             batch_size = BATCH_SIZE,\n",
    "                                             image_size = IMG_SIZE,\n",
    "                                             shuffle = True,\n",
    "                                             validation_split = 0.2,\n",
    "                                             subset = 'training',\n",
    "                                             seed = 42,\n",
    "                                             label_mode='int')\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(train_directory,\n",
    "                                                  batch_size = BATCH_SIZE,\n",
    "                                                  image_size = IMG_SIZE,\n",
    "                                                  shuffle = True,\n",
    "                                                  validation_split = 0.2,\n",
    "                                                  subset = 'validation',\n",
    "                                                  seed = 42,\n",
    "                                                  label_mode='int')\n",
    "\n",
    "test_dataset = image_dataset_from_directory(test_directory,\n",
    "                                            shuffle = False,\n",
    "                                            image_size = IMG_SIZE,\n",
    "                                            label_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d802cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPartition(Layer):\n",
    "    def __init__(self, window_size = 4, channels = 3):\n",
    "        super(PatchPartition, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            # 4x4 patches with stride of 4 for non-overlapping patches\n",
    "            sizes = [1, self.window_size, self.window_size, 1],\n",
    "            strides = [1, self.window_size, self.window_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf4be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEmbedding(Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super(LinearEmbedding, self).__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(projection_dim)\n",
    "        self.position_embedding = Embedding(input_dim = self.num_patches, output_dim = projection_dim)\n",
    "        \n",
    "    def call(self, patch):\n",
    "        patches_embed = self.projection(patch)\n",
    "        positions = tf.range(start = 0, limit = self.num_patches, delta = 1)\n",
    "        positions_embed = self.position_embedding(positions)\n",
    "        encoded = patches_embed + positions_embed\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a0a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(Layer):\n",
    "    def __init__(self, input_resolution, channels):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.channels = channels\n",
    "        self.linear_trans = Dense(2 * channels, use_bias = False)\n",
    "        \n",
    "    def call(self, x):\n",
    "        height, width = self.input_resolution\n",
    "        _, _, C = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape = (-1, height, width, C))\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = tf.concat((x0, x1, x2, x3), axis = -1)\n",
    "        x = tf.reshape(x, shape = (-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1763c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron (MLP) -- 'feedforward NN'\n",
    "class MLP(Layer):\n",
    "    def __init__(self, hidden_features, out_features, dropout_rate = 0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = Dense(hidden_features, activation = tf.nn.gelu)\n",
    "        self.dense2 = Dense(out_features)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        y = self.dropout(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f2adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(Layer):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        initializer = TruncatedNormal(mean=0., stddev=.02)\n",
    "        # position table shape is: (2*Wh-1 * 2*Ww-1, nH)\n",
    "        table_shape = ((2*self.window_size[0]-1) * (2*self.window_size[1]-1), num_heads)\n",
    "        self.relative_position_bias_table = tf.Variable(initializer(shape=table_shape))\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = tf.range(self.window_size[0])\n",
    "        coords_w = tf.range(self.window_size[1])\n",
    "        coords = tf.stack(tf.meshgrid(coords_h, coords_w))  # 2, Wh, Ww\n",
    "        coords_flatten = tf.reshape(coords, [2, -1])  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = tf.transpose(relative_coords, perm=[1,2,0]) # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords = relative_coords + [self.window_size[0] - 1, self.window_size[1] - 1]  # shift to start from 0\n",
    "        relative_coords = relative_coords * [2*self.window_size[1] - 1, 1]\n",
    "        self.relative_position_index = tf.math.reduce_sum(relative_coords,-1)  # Wh*Ww, Wh*Ww\n",
    "\n",
    "        self.qkv = Dense(dim * 3, use_bias=qkv_bias, kernel_initializer=initializer)\n",
    "        self.attn_drop = Dropout(attn_drop)\n",
    "        self.proj = Dense(dim, kernel_initializer=initializer)\n",
    "        self.proj_drop = Dropout(proj_drop)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, L, N, C = x.shape\n",
    "        qkv = tf.transpose(tf.reshape(self.qkv(x), [-1, N, 3, self.num_heads, C // self.num_heads]), perm=[2, 0, 3, 1, 4]) # [3, B_, num_head, Ww*Wh, C//num_head]\n",
    "        q, k, v = tf.unstack(qkv)  # make torchscript happy (cannot use tensor as tuple)\n",
    "        q = q * self.scale\n",
    "        attn = tf.einsum('...ij,...kj->...ik', q, k)\n",
    "        relative_position_bias = tf.reshape(tf.gather(self.relative_position_bias_table, tf.reshape(self.relative_position_index, [-1])),\n",
    "            [self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1])  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, perm=[2, 0, 1])  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0] # every window has different mask [nW, N, N]\n",
    "            attn = tf.reshape(attn, [-1 // nW, nW, self.num_heads, N, N]) + mask[:, None, :, :] # add mask: make each component -inf or just leave it\n",
    "            attn = tf.reshape(attn, [-1, self.num_heads, N, N])\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.reshape(tf.transpose(attn @ v, perm=[0, 2, 1, 3]), [-1, L, N, C])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5fc6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    _, H, W, C = x.shape\n",
    "    num_patch_y = H // window_size\n",
    "    num_patch_x = W // window_size\n",
    "    x = tf.reshape(x, [-1, num_patch_y, window_size, num_patch_x, window_size, C])\n",
    "    x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n",
    "    windows = tf.reshape(x, [-1, num_patch_x * num_patch_y, window_size, window_size, C])\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c441c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, H, W):\n",
    "    C = windows.shape[-1]\n",
    "    B = int(windows.shape[1] / (H * W / window_size / window_size))\n",
    "    x = tf.reshape(windows, [B, H // window_size, W // window_size, window_size, window_size, C])\n",
    "    x = tf.reshape(tf.transpose(x, perm=[0, 1, 3, 2, 4, 5]), [-1, H, W, C])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87102c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(Layer):\n",
    "    def __init__(self, prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if self.drop_prob == 0. or not training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = tf.random.uniform(shape=shape)\n",
    "        random_tensor = tf.where(random_tensor < keep_prob, 1, 0)\n",
    "        output = x / keep_prob * random_tensor\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21bb906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(Layer):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else tf.identity\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(mlp_hidden_dim, dim, dropout_rate=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = np.zeros([1, H, W, 1])  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            img_mask = tf.constant(img_mask)\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = tf.reshape(mask_windows, [-1, self.window_size * self.window_size])\n",
    "            attn_mask = mask_windows[:, None, :] - mask_windows[:, :, None]\n",
    "            self.attn_mask = tf.where(attn_mask==0, -100., 0.)\n",
    "        else:\n",
    "            self.attn_mask = None\n",
    "\n",
    "    def call(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, [-1, H, W, C])\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = tf.reshape(x_windows, [-1, x_windows.shape[1], self.window_size * self.window_size, C])  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = tf.reshape(attn_windows, [-1, x_windows.shape[1], self.window_size, self.window_size, C])\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = tf.reshape(x, [-1, H * W, C])\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44a6bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SwinTransformer(num_classes, input_shape=(224, 224, 3), window_size=7, embed_dim=96, num_heads=3):\n",
    "    num_patch_x = input_shape[0] // 4\n",
    "    num_patch_y = input_shape[1] // 4\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Patch Partition\n",
    "    patches = PatchPartition(window_size = 4)(inputs)\n",
    "    \n",
    "    ## Stage 1\n",
    "    # Linear Embedding\n",
    "    patches_embed = LinearEmbedding(num_patches = num_patch_x * num_patch_y, projection_dim = embed_dim)(patches)\n",
    "\n",
    "    # Swin Transformer block (first)\n",
    "    out_stage_1 = SwinTransformerBlock(\n",
    "        dim = embed_dim,\n",
    "        input_resolution = (num_patch_x, num_patch_y),\n",
    "        num_heads = num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 0\n",
    "    )(patches_embed)\n",
    "    # Swin Transformer block (second)\n",
    "    out_stage_1 = SwinTransformerBlock(\n",
    "        dim = embed_dim,\n",
    "        input_resolution = (num_patch_x, num_patch_y),\n",
    "        num_heads = num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 1\n",
    "    )(out_stage_1)\n",
    "    \n",
    "    ## Stage 2\n",
    "    # Patch Merging\n",
    "    pm_stage_2 = PatchMerging((num_patch_x, num_patch_y), channels=embed_dim)(out_stage_1)\n",
    "    \n",
    "    factor = [2 ** (i + 1) for i in range(3)]\n",
    "    \n",
    "    # Swin Transformer block (first)\n",
    "    out_stage_2 = SwinTransformerBlock(\n",
    "        dim = factor[0] * embed_dim,\n",
    "        input_resolution = (num_patch_x // factor[0], num_patch_y // factor[0]),\n",
    "        num_heads = factor[0] * num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 0\n",
    "    )(pm_stage_2)\n",
    "    # Swin Transformer block (second)\n",
    "    out_stage_2 = SwinTransformerBlock(\n",
    "        dim = factor[0] * embed_dim,\n",
    "        input_resolution = (num_patch_x // factor[0], num_patch_y // factor[0]),\n",
    "        num_heads = factor[0] * num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 1\n",
    "    )(out_stage_2)\n",
    "    \n",
    "    ## Stage 3\n",
    "    # Patch Merging\n",
    "    pm_stage_3 = PatchMerging((num_patch_x // factor[0], num_patch_y // factor[0]), channels = factor[0] * embed_dim)(out_stage_2)\n",
    "    \n",
    "    out_stage_3 = pm_stage_3\n",
    "    for _ in range(3):\n",
    "        # Swin Transformer block (1)\n",
    "        out_stage_3 = SwinTransformerBlock(\n",
    "            dim = factor[1] * embed_dim,\n",
    "            input_resolution = (num_patch_x // factor[1], num_patch_y // factor[1]),\n",
    "            num_heads = factor[1] * num_heads,\n",
    "            window_size = window_size,\n",
    "            shift_size = 0\n",
    "        )(out_stage_3)\n",
    "        # Swin Transformer block (2)\n",
    "        out_stage_3 = SwinTransformerBlock(\n",
    "            dim = factor[1] * embed_dim,\n",
    "            input_resolution = (num_patch_x // factor[1], num_patch_y // factor[1]),\n",
    "            num_heads = factor[1] * num_heads,\n",
    "            window_size = window_size,\n",
    "            shift_size = 1\n",
    "        )(out_stage_3)\n",
    "    \n",
    "    ## Stage 4\n",
    "    # Patch Merging\n",
    "    pm_stage_4 = PatchMerging((num_patch_x // factor[1], num_patch_y // factor[1]), channels = factor[1] * embed_dim)(out_stage_3)\n",
    "     \n",
    "    # Swin Transformer block (first)\n",
    "    out_stage_4 = SwinTransformerBlock(\n",
    "        dim = factor[2] * embed_dim,\n",
    "        input_resolution = (num_patch_x // factor[2], num_patch_y // factor[2]),\n",
    "        num_heads = factor[2] * num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 0\n",
    "    )(pm_stage_4)\n",
    "    # Swin Transformer block (second)\n",
    "    out_stage_4 = SwinTransformerBlock(\n",
    "        dim = factor[2] * embed_dim,\n",
    "        input_resolution = (num_patch_x // factor[2], num_patch_y // factor[2]),\n",
    "        num_heads = factor[2] * num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 1\n",
    "    )(out_stage_4)\n",
    "    \n",
    "    # pooling\n",
    "    representation = GlobalAveragePooling1D()(out_stage_4)\n",
    "    # logits\n",
    "    output = Dense(num_classes, activation=\"softmax\")(representation)\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9fda2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandramarcelino/jupyter_venv/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = create_SwinTransformer(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf574ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " patch_partition (PatchPart  (None, None, 48)          0         \n",
      " ition)                                                          \n",
      "                                                                 \n",
      " linear_embedding (LinearEm  (None, 3136, 96)          305760    \n",
      " bedding)                                                        \n",
      "                                                                 \n",
      " swin_transformer_block (Sw  (None, 3136, 96)          112347    \n",
      " inTransformerBlock)                                             \n",
      "                                                                 \n",
      " swin_transformer_block_1 (  (None, 3136, 96)          112347    \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " patch_merging (PatchMergin  (None, 784, 192)          73728     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " swin_transformer_block_2 (  (None, 784, 192)          445878    \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_3 (  (None, 784, 192)          445878    \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " patch_merging_1 (PatchMerg  (None, 196, 384)          294912    \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " swin_transformer_block_4 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_5 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_6 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_7 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_8 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_9 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " patch_merging_2 (PatchMerg  (None, 49, 768)           1179648   \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " swin_transformer_block_10   (None, 49, 768)           7091928   \n",
      " (SwinTransformerBlock)                                          \n",
      "                                                                 \n",
      " swin_transformer_block_11   (None, 49, 768)           7091928   \n",
      " (SwinTransformerBlock)                                          \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 768)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 4)                 3076      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27816382 (106.11 MB)\n",
      "Trainable params: 27816382 (106.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e03ee4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputLayer\n",
      "PatchPartition\n",
      "LinearEmbedding\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "PatchMerging\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "PatchMerging\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "PatchMerging\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "GlobalAveragePooling1D\n",
      "Dense\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680450d",
   "metadata": {},
   "source": [
    "###### We should ideally load the weights of the pretrained model to ensure high accuracy/performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "604d3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.models.load_model(notebook_path + '/swin_tiny', compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a5b46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights\n",
    "pretrained_weights = pretrained_model.get_weights()"
   ]
  },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
