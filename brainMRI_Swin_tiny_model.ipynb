{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e8b743",
   "metadata": {},
   "source": [
    "### [IN PROGRESS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30971007",
   "metadata": {},
   "source": [
    "Sources used in this notebook:\\\n",
    "    1. [This google colab that implements a simpler version](https://colab.research.google.com/github/dzlab/notebooks/blob/master/_notebooks/2022-02-27-Swin_Transfomer.ipynb#scrollTo=-UnAWaONhf9J)\\\n",
    "    2. [The model outlined in the original research paper](https://arxiv.org/pdf/2103.14030.pdf)\\\n",
    "    3. [To load the weights of the pretrained model](https://tfhub.dev/sayakpaul/swin_s3_tiny_224/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbae09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention,\n",
    "    Softmax\n",
    ")\n",
    "from tensorflow.keras.initializers import TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b56833",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.path.dirname(os.path.abspath('brain_MRI_classification.ipynb'))\n",
    "datasets_combined = os.path.join(notebook_path, 'brainMRI_data')\n",
    "\n",
    "train_directory = os.path.join(datasets_combined, 'Training')\n",
    "test_directory = os.path.join(datasets_combined, 'Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52809fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 files belonging to 4 classes.\n",
      "Using 2296 files for training.\n",
      "Found 2870 files belonging to 4 classes.\n",
      "Using 574 files for validation.\n",
      "Found 394 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# using 'int' to use sparse_categorical_crossentropy for loss\n",
    "train_dataset = image_dataset_from_directory(train_directory,\n",
    "                                             batch_size = BATCH_SIZE,\n",
    "                                             image_size = IMG_SIZE,\n",
    "                                             shuffle = True,\n",
    "                                             validation_split = 0.2,\n",
    "                                             subset = 'training',\n",
    "                                             seed = 42,\n",
    "                                             label_mode='int')\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(train_directory,\n",
    "                                                  batch_size = BATCH_SIZE,\n",
    "                                                  image_size = IMG_SIZE,\n",
    "                                                  shuffle = True,\n",
    "                                                  validation_split = 0.2,\n",
    "                                                  subset = 'validation',\n",
    "                                                  seed = 42,\n",
    "                                                  label_mode='int')\n",
    "\n",
    "test_dataset = image_dataset_from_directory(test_directory,\n",
    "                                            shuffle = False,\n",
    "                                            image_size = IMG_SIZE,\n",
    "                                            label_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d802cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPartition(Layer):\n",
    "    def __init__(self, window_size = 4, channels = 3):\n",
    "        super(PatchPartition, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            # 4x4 patches with stride of 4 for non-overlapping patches\n",
    "            sizes = [1, self.window_size, self.window_size, 1],\n",
    "            strides = [1, self.window_size, self.window_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf4be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEmbedding(Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super(LinearEmbedding, self).__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(projection_dim)\n",
    "        self.position_embedding = Embedding(input_dim = self.num_patches, output_dim = projection_dim)\n",
    "        \n",
    "    def call(self, patch):\n",
    "        patches_embed = self.projection(patch)\n",
    "        positions = tf.range(start = 0, limit = self.num_patches, delta = 1)\n",
    "        positions_embed = self.position_embedding(positions)\n",
    "        encoded = patches_embed + positions_embed\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a0a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(Layer):\n",
    "    def __init__(self, input_resolution, channels):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.channels = channels\n",
    "        self.linear_trans = Dense(2 * channels, use_bias = False)\n",
    "        \n",
    "    def call(self, x):\n",
    "        height, width = self.input_resolution\n",
    "        _, _, C = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape = (-1, height, width, C))\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = tf.concat((x0, x1, x2, x3), axis = -1)\n",
    "        x = tf.reshape(x, shape = (-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1763c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron (MLP) -- 'feedforward NN'\n",
    "class MLP(Layer):\n",
    "    def __init__(self, hidden_features, out_features, dropout_rate = 0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = Dense(hidden_features, activation = tf.nn.gelu)\n",
    "        self.dense2 = Dense(out_features)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        y = self.dropout(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f2adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(Layer):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        initializer = TruncatedNormal(mean=0., stddev=.02)\n",
    "        # position table shape is: (2*Wh-1 * 2*Ww-1, nH)\n",
    "        table_shape = ((2*self.window_size[0]-1) * (2*self.window_size[1]-1), num_heads)\n",
    "        self.relative_position_bias_table = tf.Variable(initializer(shape=table_shape))\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = tf.range(self.window_size[0])\n",
    "        coords_w = tf.range(self.window_size[1])\n",
    "        coords = tf.stack(tf.meshgrid(coords_h, coords_w))  # 2, Wh, Ww\n",
    "        coords_flatten = tf.reshape(coords, [2, -1])  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = tf.transpose(relative_coords, perm=[1,2,0]) # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords = relative_coords + [self.window_size[0] - 1, self.window_size[1] - 1]  # shift to start from 0\n",
    "        relative_coords = relative_coords * [2*self.window_size[1] - 1, 1]\n",
    "        self.relative_position_index = tf.math.reduce_sum(relative_coords,-1)  # Wh*Ww, Wh*Ww\n",
    "\n",
    "        self.qkv = Dense(dim * 3, use_bias=qkv_bias, kernel_initializer=initializer)\n",
    "        self.attn_drop = Dropout(attn_drop)\n",
    "        self.proj = Dense(dim, kernel_initializer=initializer)\n",
    "        self.proj_drop = Dropout(proj_drop)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, L, N, C = x.shape\n",
    "        qkv = tf.transpose(tf.reshape(self.qkv(x), [-1, N, 3, self.num_heads, C // self.num_heads]), perm=[2, 0, 3, 1, 4]) # [3, B_, num_head, Ww*Wh, C//num_head]\n",
    "        q, k, v = tf.unstack(qkv)  # make torchscript happy (cannot use tensor as tuple)\n",
    "        q = q * self.scale\n",
    "        attn = tf.einsum('...ij,...kj->...ik', q, k)\n",
    "        relative_position_bias = tf.reshape(tf.gather(self.relative_position_bias_table, tf.reshape(self.relative_position_index, [-1])),\n",
    "            [self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1])  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, perm=[2, 0, 1])  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0] # every window has different mask [nW, N, N]\n",
    "            attn = tf.reshape(attn, [-1 // nW, nW, self.num_heads, N, N]) + mask[:, None, :, :] # add mask: make each component -inf or just leave it\n",
    "            attn = tf.reshape(attn, [-1, self.num_heads, N, N])\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.reshape(tf.transpose(attn @ v, perm=[0, 2, 1, 3]), [-1, L, N, C])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5fc6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    _, H, W, C = x.shape\n",
    "    num_patch_y = H // window_size\n",
    "    num_patch_x = W // window_size\n",
    "    x = tf.reshape(x, [-1, num_patch_y, window_size, num_patch_x, window_size, C])\n",
    "    x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n",
    "    windows = tf.reshape(x, [-1, num_patch_x * num_patch_y, window_size, window_size, C])\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c441c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size, H, W):\n",
    "    C = windows.shape[-1]\n",
    "    B = int(windows.shape[1] / (H * W / window_size / window_size))\n",
    "    x = tf.reshape(windows, [B, H // window_size, W // window_size, window_size, window_size, C])\n",
    "    x = tf.reshape(tf.transpose(x, perm=[0, 1, 3, 2, 4, 5]), [-1, H, W, C])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87102c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(Layer):\n",
    "    def __init__(self, prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if self.drop_prob == 0. or not training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = tf.random.uniform(shape=shape)\n",
    "        random_tensor = tf.where(random_tensor < keep_prob, 1, 0)\n",
    "        output = x / keep_prob * random_tensor\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21bb906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(Layer):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else tf.identity\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-5)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(mlp_hidden_dim, dim, dropout_rate=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = np.zeros([1, H, W, 1])  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            img_mask = tf.constant(img_mask)\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = tf.reshape(mask_windows, [-1, self.window_size * self.window_size])\n",
    "            attn_mask = mask_windows[:, None, :] - mask_windows[:, :, None]\n",
    "            self.attn_mask = tf.where(attn_mask==0, -100., 0.)\n",
    "        else:\n",
    "            self.attn_mask = None\n",
    "\n",
    "    def call(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, [-1, H, W, C])\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = tf.reshape(x_windows, [-1, x_windows.shape[1], self.window_size * self.window_size, C])  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = tf.reshape(attn_windows, [-1, x_windows.shape[1], self.window_size, self.window_size, C])\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = tf.reshape(x, [-1, H * W, C])\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44a6bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SwinTransformer(num_classes, input_shape=(224, 224, 3), window_size=7, embed_dim=96, num_heads=3):\n",
    "    num_patch_x = input_shape[0] // 4\n",
    "    num_patch_y = input_shape[1] // 4\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Patch Partition\n",
    "    patches = PatchPartition(window_size = 4)(inputs)\n",
    "    \n",
    "    ## Stage 1\n",
    "    # Linear Embedding\n",
    "    patches_embed = LinearEmbedding(num_patches = num_patch_x * num_patch_y, projection_dim = embed_dim)(patches)\n",
    "\n",
    "    # Swin Transformer block (first)\n",
    "    out_stage_1 = SwinTransformerBlock(\n",
    "        dim = embed_dim,\n",
    "        input_resolution = (num_patch_x, num_patch_y),\n",
    "        num_heads = num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 0\n",
    "    )(patches_embed)\n",
    "    # Swin Transformer block (second)\n",
    "    out_stage_1 = SwinTransformerBlock(\n",
    "        dim = embed_dim,\n",
    "        input_resolution = (num_patch_x, num_patch_y),\n",
    "        num_heads = num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 1\n",
    "    )(out_stage_1)\n",
    "    \n",
    "    ## Stage 2\n",
    "    # Patch Merging\n",
    "    pm_stage_2 = PatchMerging((num_patch_x, num_patch_y), channels=embed_dim)(out_stage_1)\n",
    "    \n",
    "    factor = [2 ** (i + 1) for i in range(3)]\n",
    "    \n",
    "    # Swin Transformer block (first)\n",
    "    out_stage_2 = SwinTransformerBlock(\n",
    "        dim = factor[0] * embed_dim,\n",
    "        input_resolution = (num_patch_x // factor[0], num_patch_y // factor[0]),\n",
    "        num_heads = factor[0] * num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 0\n",
    "    )(pm_stage_2)\n",
    "    # Swin Transformer block (second)\n",
    "    out_stage_2 = SwinTransformerBlock(\n",
    "        dim = factor[0] * embed_dim,\n",
    "        input_resolution = (num_patch_x // factor[0], num_patch_y // factor[0]),\n",
    "        num_heads = factor[0] * num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 1\n",
    "    )(out_stage_2)\n",
    "    \n",
    "    ## Stage 3\n",
    "    # Patch Merging\n",
    "    pm_stage_3 = PatchMerging((num_patch_x // factor[0], num_patch_y // factor[0]), channels = factor[0] * embed_dim)(out_stage_2)\n",
    "    \n",
    "    out_stage_3 = pm_stage_3\n",
    "    for _ in range(3):\n",
    "        # Swin Transformer block (1)\n",
    "        out_stage_3 = SwinTransformerBlock(\n",
    "            dim = factor[1] * embed_dim,\n",
    "            input_resolution = (num_patch_x // factor[1], num_patch_y // factor[1]),\n",
    "            num_heads = factor[1] * num_heads,\n",
    "            window_size = window_size,\n",
    "            shift_size = 0\n",
    "        )(out_stage_3)\n",
    "        # Swin Transformer block (2)\n",
    "        out_stage_3 = SwinTransformerBlock(\n",
    "            dim = factor[1] * embed_dim,\n",
    "            input_resolution = (num_patch_x // factor[1], num_patch_y // factor[1]),\n",
    "            num_heads = factor[1] * num_heads,\n",
    "            window_size = window_size,\n",
    "            shift_size = 1\n",
    "        )(out_stage_3)\n",
    "    \n",
    "    ## Stage 4\n",
    "    # Patch Merging\n",
    "    pm_stage_4 = PatchMerging((num_patch_x // factor[1], num_patch_y // factor[1]), channels = factor[1] * embed_dim)(out_stage_3)\n",
    "     \n",
    "    # Swin Transformer block (first)\n",
    "    out_stage_4 = SwinTransformerBlock(\n",
    "        dim = factor[2] * embed_dim,\n",
    "        input_resolution = (num_patch_x // factor[2], num_patch_y // factor[2]),\n",
    "        num_heads = factor[2] * num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 0\n",
    "    )(pm_stage_4)\n",
    "    # Swin Transformer block (second)\n",
    "    out_stage_4 = SwinTransformerBlock(\n",
    "        dim = factor[2] * embed_dim,\n",
    "        input_resolution = (num_patch_x // factor[2], num_patch_y // factor[2]),\n",
    "        num_heads = factor[2] * num_heads,\n",
    "        window_size = window_size,\n",
    "        shift_size = 1\n",
    "    )(out_stage_4)\n",
    "    \n",
    "    # pooling\n",
    "    representation = GlobalAveragePooling1D()(out_stage_4)\n",
    "    # logits\n",
    "    output = Dense(num_classes, activation=\"softmax\")(representation)\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9fda2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandramarcelino/jupyter_venv/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = create_SwinTransformer(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf574ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " patch_partition (PatchPart  (None, None, 48)          0         \n",
      " ition)                                                          \n",
      "                                                                 \n",
      " linear_embedding (LinearEm  (None, 3136, 96)          305760    \n",
      " bedding)                                                        \n",
      "                                                                 \n",
      " swin_transformer_block (Sw  (None, 3136, 96)          112347    \n",
      " inTransformerBlock)                                             \n",
      "                                                                 \n",
      " swin_transformer_block_1 (  (None, 3136, 96)          112347    \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " patch_merging (PatchMergin  (None, 784, 192)          73728     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " swin_transformer_block_2 (  (None, 784, 192)          445878    \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_3 (  (None, 784, 192)          445878    \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " patch_merging_1 (PatchMerg  (None, 196, 384)          294912    \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " swin_transformer_block_4 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_5 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_6 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_7 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_8 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " swin_transformer_block_9 (  (None, 196, 384)          1776492   \n",
      " SwinTransformerBlock)                                           \n",
      "                                                                 \n",
      " patch_merging_2 (PatchMerg  (None, 49, 768)           1179648   \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " swin_transformer_block_10   (None, 49, 768)           7091928   \n",
      " (SwinTransformerBlock)                                          \n",
      "                                                                 \n",
      " swin_transformer_block_11   (None, 49, 768)           7091928   \n",
      " (SwinTransformerBlock)                                          \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 768)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 4)                 3076      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27816382 (106.11 MB)\n",
      "Trainable params: 27816382 (106.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e03ee4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputLayer\n",
      "PatchPartition\n",
      "LinearEmbedding\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "PatchMerging\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "PatchMerging\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "PatchMerging\n",
      "SwinTransformerBlock\n",
      "SwinTransformerBlock\n",
      "GlobalAveragePooling1D\n",
      "Dense\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680450d",
   "metadata": {},
   "source": [
    "###### We should ideally load the weights of the pretrained model to ensure high accuracy/performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "604d3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.models.load_model(notebook_path + '/swin_tiny', compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a5b46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights\n",
    "pretrained_weights = pretrained_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1119be66",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"model\" with a weight list of length 173, but the layer was expecting 164 weights. Provided weights: [array([[[[ 2.49835104e-02,  2.71712639e-03, -5.77...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set the initial weights to the pre-trained weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_venv/lib/python3.11/site-packages/keras/src/engine/base_layer.py:1807\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         expected_num_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_num_weights \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m-> 1807\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou called `set_weights(weights)` on layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1809\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a weight list of length \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, but the layer was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1810\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m weights. Provided weights: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1811\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1813\u001b[0m             \u001b[38;5;28mlen\u001b[39m(weights),\n\u001b[1;32m   1814\u001b[0m             expected_num_weights,\n\u001b[1;32m   1815\u001b[0m             \u001b[38;5;28mstr\u001b[39m(weights)[:\u001b[38;5;241m50\u001b[39m],\n\u001b[1;32m   1816\u001b[0m         )\n\u001b[1;32m   1817\u001b[0m     )\n\u001b[1;32m   1819\u001b[0m weight_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1820\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"model\" with a weight list of length 173, but the layer was expecting 164 weights. Provided weights: [array([[[[ 2.49835104e-02,  2.71712639e-03, -5.77..."
     ]
    }
   ],
   "source": [
    "# Set the initial weights to the pre-trained weights\n",
    "model.set_weights(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1189b",
   "metadata": {},
   "source": [
    "We come across an issue with a mismatch in dimensions for the weights, where the pretrained weights have dimension 173, but our current model expects length of 164."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ad1b8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var in model.trainable_variables:\n",
    "#     print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9f37fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var in pretrained_model.trainable_variables:\n",
    "#     print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fc680f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                                                                                          pretrained_model \n",
      "\n",
      "linear_embedding/embedding/embeddings:0                                             layer_normalization/gamma:0\n",
      "                                                                                     layer_normalization/beta:0\n",
      "\n",
      "\n",
      "                                                                    patch_merging/layer_normalization_5/gamma:0\n",
      "                                                                     patch_merging/layer_normalization_5/beta:0\n",
      "patch_merging/dense_9/kernel:0                                                   patch_merging/dense_4/kernel:0\n",
      "\n",
      "\n",
      "                                                                 patch_merging_2/layer_normalization_23/gamma:0\n",
      "                                                                  patch_merging_2/layer_normalization_23/beta:0\n",
      "patch_merging_2/dense_43/kernel:0                                             patch_merging_2/dense_22/kernel:0\n",
      "\n",
      "dense_52/kernel:0                                   swin_tiny_patch4_window7_224/layer_normalization_28/gamma:0\n",
      "dense_52/bias:0                                      swin_tiny_patch4_window7_224/layer_normalization_28/beta:0\n",
      "                                                      swin_tiny_patch4_window7_224/classification_head/kernel:0\n",
      "                                                        swin_tiny_patch4_window7_224/classification_head/bias:0\n"
     ]
    }
   ],
   "source": [
    "model_vars = model.trainable_variables\n",
    "pretrained_model_vars = pretrained_model.trainable_variables\n",
    "print(f\"{'model' :<55}{'pretrained_model' :>56} \\n\")\n",
    "\n",
    "##\n",
    "short_model_var = '/'.join(model_vars[2].name.split('/')[-3:])\n",
    "short_pmodel_var = '/'.join(pretrained_model_vars[2].name.split('/')[-3:])\n",
    "print('{:<55}{:>56}'.format(short_model_var, short_pmodel_var))\n",
    "print('{:>111}\\n'.format('/'.join(pretrained_model_vars[3].name.split('/')[-3:])))\n",
    "\n",
    "##   \n",
    "print('\\n{:>111}'.format('/'.join(pretrained_model_vars[30].name.split('/')[-3:])))\n",
    "print('{:>111}'.format('/'.join(pretrained_model_vars[31].name.split('/')[-3:])))\n",
    "short_model_var = '/'.join(model_vars[29].name.split('/')[-3:])\n",
    "short_pmodel_var = '/'.join(pretrained_model_vars[32].name.split('/')[-3:])\n",
    "print('{:<55}{:>56}\\n'.format(short_model_var, short_pmodel_var))\n",
    "\n",
    "##\n",
    "print('\\n{:>111}'.format('/'.join(pretrained_model_vars[140].name.split('/')[-3:])))\n",
    "print('{:>111}'.format('/'.join(pretrained_model_vars[141].name.split('/')[-3:])))\n",
    "short_model_var = '/'.join(model_vars[135].name.split('/')[-3:])\n",
    "short_pmodel_var = '/'.join(pretrained_model_vars[142].name.split('/')[-3:])\n",
    "print('{:<55}{:>56}\\n'.format(short_model_var, short_pmodel_var))\n",
    "\n",
    "##\n",
    "for i in range(169, 171):\n",
    "    short_model_var = '/'.join(model_vars[i-7].name.split('/')[-3:])\n",
    "    short_pmodel_var = '/'.join(pretrained_model_vars[i].name.split('/')[-3:])\n",
    "    print('{:<50}{:>61}'.format(short_model_var, short_pmodel_var))\n",
    "print('{:>111}'.format('/'.join(pretrained_model_vars[171].name.split('/')[-3:])))\n",
    "print('{:>111}'.format('/'.join(pretrained_model_vars[172].name.split('/')[-3:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395db6ce",
   "metadata": {},
   "source": [
    "The differences I can spot:\\\n",
    "    - Our model has a linear embedding layer with weights of shape (3136, 96), whereas the pretrained model has the weights and biases for a normalization layer instead, each tensors of shape (96).\\\n",
    "    - For 'Patch Merging', the pretrained model has weights and biases for a normalization layer in addition to the dense layer, wheras our model only has the dense layer. Since there are 3 patch merging layers, the pretrained model would have 3(2) = 6 of the patch merging normalization layers in addition to the dense layer.\\\n",
    "    - Instead of the weights and biases for the dense layer at the end in our model, the pretrained has weights and biases for normalization layer and classification layer.\n",
    "\n",
    "So 1 + 6 + (2(2)-2) = 9 accounts for all the additional weights in the pretrained model. We would have to make changes in our original model or change the format of the pretrained weights in order for the tensors to have the same shape, so that we can load the weights onto our model. It would make a significant difference in our classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb5e51",
   "metadata": {},
   "source": [
    "We can alternatively get the model from installing an additional library, like Torch(pytorch), or installing tfswin, the keras implementation of the pytorch model. Either of these may be more efficient. Though, doing the above was helpful in terms of learning about what the model is comprised of, its archictecture, what each step is doing, and what we should expect of our inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d18a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
