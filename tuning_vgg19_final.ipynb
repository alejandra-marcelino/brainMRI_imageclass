{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cc19cd",
   "metadata": {},
   "source": [
    "# Detecting brain tumors on MRI images using VGG19 model and tuning its hyperparameters to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4ca043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Dropout, Rescaling\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08be3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.path.dirname(os.path.abspath('brain_MRI_classification.ipynb'))\n",
    "datasets_combined = os.path.join(notebook_path, 'brainMRI_data')\n",
    "\n",
    "train_directory = os.path.join(datasets_combined, 'Training')\n",
    "test_directory = os.path.join(datasets_combined, 'Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10483e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 files belonging to 4 classes.\n",
      "Using 2296 files for training.\n",
      "Found 2870 files belonging to 4 classes.\n",
      "Using 574 files for validation.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "train_dataset = image_dataset_from_directory(train_directory,\n",
    "                                             batch_size = BATCH_SIZE,\n",
    "                                             image_size = IMG_SIZE,\n",
    "                                             shuffle = True,\n",
    "                                             validation_split = 0.2,\n",
    "                                             subset = 'training',\n",
    "                                             seed = 42,\n",
    "                                             label_mode='categorical')\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(train_directory,\n",
    "                                                  batch_size = BATCH_SIZE,\n",
    "                                                  image_size = IMG_SIZE,\n",
    "                                                  shuffle = True,\n",
    "                                                  validation_split = 0.2,\n",
    "                                                  subset = 'validation',\n",
    "                                                  seed = 42,\n",
    "                                                  label_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e921964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = image_dataset_from_directory(test_directory,\n",
    "                                            shuffle = False,\n",
    "                                            image_size = IMG_SIZE,\n",
    "                                            label_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15bd11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a7780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (224, 224) -> (224, 224, 3) for the 3 color channels\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "# load network without the top classification layers\n",
    "base_model = VGG19(include_top = False,\n",
    "                   weights = 'imagenet',\n",
    "                   input_shape=IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe769f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomRotation(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85efc5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only augment training set\n",
    "train_augmented = train_dataset.map(lambda x, y: (data_augmentation(x, training = False), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab93fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.vgg19.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cad119",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# Flatten the output layer to 1D\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Add a fully connected layer with 4096 hidden units, ReLU activation\n",
    "x = Dense(4096, activation = 'relu')(x)\n",
    "\n",
    "# Add a dropout layer with 0.2 (20%) rate\n",
    "x = Dropout(0.5)(x) \n",
    "\n",
    "# Add another FC layer, 4096 units, ReLU activation\n",
    "x = Dense(4096, activation = 'relu')(x)\n",
    "\n",
    "# Add another dropout layer with 0.2 (20%) rate\n",
    "x = Dropout(0.5)(x) \n",
    "\n",
    "# Add a final FC layer for classification with 4 units using softmax activation function\n",
    "outputs = Dense(4, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8a02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and compile the model\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2054043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "36/36 [==============================] - 349s 10s/step - loss: 7.4986 - accuracy: 0.6485 - val_loss: 1.5439 - val_accuracy: 0.8571\n",
      "Epoch 2/40\n",
      "36/36 [==============================] - 359s 10s/step - loss: 2.0564 - accuracy: 0.8423 - val_loss: 0.8211 - val_accuracy: 0.8937\n",
      "Epoch 3/40\n",
      "36/36 [==============================] - 385s 11s/step - loss: 1.0245 - accuracy: 0.9007 - val_loss: 1.3292 - val_accuracy: 0.8728\n",
      "Epoch 4/40\n",
      "36/36 [==============================] - 367s 10s/step - loss: 0.9446 - accuracy: 0.9133 - val_loss: 0.6652 - val_accuracy: 0.9199\n",
      "Epoch 5/40\n",
      "36/36 [==============================] - 369s 10s/step - loss: 0.6982 - accuracy: 0.9312 - val_loss: 1.3218 - val_accuracy: 0.8902\n",
      "Epoch 6/40\n",
      "36/36 [==============================] - 372s 10s/step - loss: 0.6316 - accuracy: 0.9425 - val_loss: 0.5891 - val_accuracy: 0.9338\n",
      "Epoch 7/40\n",
      "36/36 [==============================] - 376s 10s/step - loss: 0.3722 - accuracy: 0.9551 - val_loss: 1.5062 - val_accuracy: 0.8798\n",
      "Epoch 8/40\n",
      "36/36 [==============================] - 378s 11s/step - loss: 0.3231 - accuracy: 0.9639 - val_loss: 0.6202 - val_accuracy: 0.9233\n",
      "Epoch 9/40\n",
      "36/36 [==============================] - 381s 11s/step - loss: 0.2286 - accuracy: 0.9699 - val_loss: 1.2652 - val_accuracy: 0.9094\n",
      "Epoch 10/40\n",
      "36/36 [==============================] - 387s 11s/step - loss: 0.2367 - accuracy: 0.9726 - val_loss: 1.2028 - val_accuracy: 0.9146\n",
      "Epoch 11/40\n",
      "36/36 [==============================] - 389s 11s/step - loss: 0.2269 - accuracy: 0.9704 - val_loss: 1.7838 - val_accuracy: 0.8990\n",
      "Epoch 12/40\n",
      "36/36 [==============================] - 386s 11s/step - loss: 0.2760 - accuracy: 0.9782 - val_loss: 1.2696 - val_accuracy: 0.9059\n",
      "Epoch 13/40\n",
      "36/36 [==============================] - 387s 11s/step - loss: 0.2265 - accuracy: 0.9769 - val_loss: 1.0233 - val_accuracy: 0.9303\n",
      "Epoch 14/40\n",
      "36/36 [==============================] - 389s 11s/step - loss: 0.3874 - accuracy: 0.9665 - val_loss: 1.0182 - val_accuracy: 0.9408\n",
      "Epoch 15/40\n",
      "36/36 [==============================] - 394s 11s/step - loss: 0.2682 - accuracy: 0.9734 - val_loss: 0.8690 - val_accuracy: 0.9390\n",
      "Epoch 16/40\n",
      "36/36 [==============================] - 384s 11s/step - loss: 0.1903 - accuracy: 0.9774 - val_loss: 1.3406 - val_accuracy: 0.9164\n",
      "Epoch 17/40\n",
      "36/36 [==============================] - 377s 11s/step - loss: 0.1984 - accuracy: 0.9826 - val_loss: 2.1511 - val_accuracy: 0.9024\n",
      "Epoch 18/40\n",
      "36/36 [==============================] - 374s 10s/step - loss: 0.1662 - accuracy: 0.9843 - val_loss: 1.2230 - val_accuracy: 0.9216\n",
      "Epoch 19/40\n",
      "36/36 [==============================] - 375s 10s/step - loss: 0.1812 - accuracy: 0.9843 - val_loss: 1.2709 - val_accuracy: 0.9251\n",
      "Epoch 20/40\n",
      "36/36 [==============================] - 374s 10s/step - loss: 0.1892 - accuracy: 0.9834 - val_loss: 1.6930 - val_accuracy: 0.9077\n",
      "Epoch 21/40\n",
      "36/36 [==============================] - 372s 10s/step - loss: 0.1757 - accuracy: 0.9852 - val_loss: 1.2523 - val_accuracy: 0.9338\n",
      "Epoch 22/40\n",
      "36/36 [==============================] - 371s 10s/step - loss: 0.1432 - accuracy: 0.9865 - val_loss: 1.0791 - val_accuracy: 0.9390\n",
      "Epoch 23/40\n",
      "36/36 [==============================] - 369s 10s/step - loss: 0.1879 - accuracy: 0.9856 - val_loss: 3.0463 - val_accuracy: 0.9077\n",
      "Epoch 24/40\n",
      "36/36 [==============================] - 372s 10s/step - loss: 0.1748 - accuracy: 0.9891 - val_loss: 1.6703 - val_accuracy: 0.9268\n",
      "Epoch 25/40\n",
      "36/36 [==============================] - 371s 10s/step - loss: 0.2145 - accuracy: 0.9791 - val_loss: 1.6251 - val_accuracy: 0.9077\n",
      "Epoch 26/40\n",
      "36/36 [==============================] - 370s 10s/step - loss: 0.2374 - accuracy: 0.9821 - val_loss: 1.5370 - val_accuracy: 0.9321\n",
      "Epoch 27/40\n",
      "36/36 [==============================] - 371s 10s/step - loss: 0.1270 - accuracy: 0.9917 - val_loss: 1.2043 - val_accuracy: 0.9321\n",
      "Epoch 28/40\n",
      "36/36 [==============================] - 1096s 31s/step - loss: 0.0958 - accuracy: 0.9909 - val_loss: 1.7244 - val_accuracy: 0.9286\n",
      "Epoch 29/40\n",
      "36/36 [==============================] - 4264s 122s/step - loss: 0.3009 - accuracy: 0.9813 - val_loss: 2.8365 - val_accuracy: 0.8990\n",
      "Epoch 30/40\n",
      "36/36 [==============================] - 417s 12s/step - loss: 0.3395 - accuracy: 0.9800 - val_loss: 1.9897 - val_accuracy: 0.9268\n",
      "Epoch 31/40\n",
      "36/36 [==============================] - 2471s 70s/step - loss: 0.1521 - accuracy: 0.9895 - val_loss: 2.7400 - val_accuracy: 0.9129\n",
      "Epoch 32/40\n",
      "36/36 [==============================] - 1568s 45s/step - loss: 0.3614 - accuracy: 0.9800 - val_loss: 2.5550 - val_accuracy: 0.9251\n",
      "Epoch 33/40\n",
      "36/36 [==============================] - 6129s 175s/step - loss: 0.1658 - accuracy: 0.9891 - val_loss: 2.3326 - val_accuracy: 0.9286\n",
      "Epoch 34/40\n",
      "36/36 [==============================] - 4051s 115s/step - loss: 0.1254 - accuracy: 0.9904 - val_loss: 2.1754 - val_accuracy: 0.9321\n",
      "Epoch 35/40\n",
      "36/36 [==============================] - 997s 28s/step - loss: 0.1015 - accuracy: 0.9917 - val_loss: 1.9444 - val_accuracy: 0.9251\n",
      "Epoch 36/40\n",
      "36/36 [==============================] - 335s 9s/step - loss: 0.0938 - accuracy: 0.9943 - val_loss: 1.4181 - val_accuracy: 0.9373\n",
      "Epoch 37/40\n",
      "36/36 [==============================] - 4179s 119s/step - loss: 0.0798 - accuracy: 0.9935 - val_loss: 2.0622 - val_accuracy: 0.9355\n",
      "Epoch 38/40\n",
      "36/36 [==============================] - 2522s 72s/step - loss: 0.1164 - accuracy: 0.9922 - val_loss: 2.7672 - val_accuracy: 0.9094\n",
      "Epoch 39/40\n",
      "36/36 [==============================] - 335s 9s/step - loss: 0.0653 - accuracy: 0.9939 - val_loss: 2.6768 - val_accuracy: 0.9216\n",
      "Epoch 40/40\n",
      "36/36 [==============================] - 347s 10s/step - loss: 0.1664 - accuracy: 0.9913 - val_loss: 1.7613 - val_accuracy: 0.9233\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(train_augmented, epochs = 40, validation_data = validation_dataset, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68124331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 50s 4s/step - loss: 29.2375 - accuracy: 0.7360\n",
      "Test accuracy : 0.7360405921936035\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf747b0",
   "metadata": {},
   "source": [
    "<pre>\n",
    "While training the model, the accuracy came out to 99.13% for the training set and 92.33% for the validation set, after epoch 40/40. However, when evaluating the mdoel against the testing set, accuracy came to about 73.6%. \n",
    "\n",
    "We took different approaches to tune the model in order to improve its performance. It is important to note that the process involved using the strategy of \"orthogonalization\" in the context of tuning hyperparameters, though not all the models and results are shown. We optimized the model in the following ways (not necessarily in the order given):\n",
    "\n",
    "We preprocessed the data the same way the pretrained model processed the ImageNet data to get the learned weights. Based on tensorflow documentation, the images are converted from RGB to BGR and each color channel is zero-centered based on the ImageNet data. This normalizes the data, which helps the learning algorithm converge faster. We lowered the learning rate from 0.001 (default) to 0.0001 while using the Adam optimization algorithm, which allowed us to take advantage of the techniques behind momentum and RMSprop. We lowered this parameter when the training accuracy was relatively low in attempts to increase it. Additionally, the training time was increased from 20 epochs to 40 epochs (ultimate choice was influenced by research papers), which was also done in attempt to increase the learning accuracy for the training set. With a combination of these, the training accuracy increased significantly based on previous drafts of the model, but some methods compromised the validation accuracy slightly. \n",
    "\n",
    "Additionally, we performed data augmentation as a method of regularization in order to avoid overfitting the data and decrease variance. Implementing the Dropout layers with a 50% rate was also a method of decreasing variance. We lowered the learning rate from 0.001 (default) to 0.0001 while using the Adam optimization algorithm, which allowed us to take advantage of the techniques behind momentum and RMSprop. We increased the batch size from 32 to 64, which helped the model converge more quickly, but may have compromised the long term learning accuracy, potentionally contibuting to the ongoing fluctuations in accuracy/losses. However it's important to note that a combination of some of the methods mentioned above decreased the extent of this fluctuation/oscillation in the latter epochs.\n",
    "\n",
    "There are many approaches we can take to further improve the model's performance. We can see how the model performs using batch size of 32 with the rest of the parameters fixed, since this change was made early on. This would increase the training time, however, and given how long it currently takes to run, that's something I would consider after trying out other approaches. The relatively high accuracy for the training and validation set compared to that of the testing set indicates that the model may have overfit the data and is unable to generalize when given new data. Thus, a promising approach would be to further consider implementing other methods of regularization.\n",
    "\n",
    "We will attempt to train a Transformer network (Swin) to compare its performance on the testing set with what we see here.\n",
    "\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c6211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
